all(ints > 0)
Sys.Date()
mean(c(2, 4, 5))
submit()
boring_function('My first function!')
boring_function
submit()
my_mean(c(4, 5, 10))
submit()
remainder(5)
remainder(11, 5)
remainder(divisor = 11, num = 5)
remainder(4, div = 2)
args(remainder)
submit()
evaluate(median, c(1.4, 3.6, 7.9, 8.8))
evaluate(sd, c(1.4, 3.6, 7.9, 8.8))
evaluate(function (x) {x+1}, 6)
evaluate(function (x) {x[1]}, c(8, 4, 0))
evaluate(function (x) {x[3]}, c(8, 4, 0))
evaluate(function (x) {x[length(x)]}, c(8, 4, 0))
?paste
paste("Programming", "is", "fun!")
submit()
telegram("HELL", "NO")
submit()
mad_libs(place = 'place', adjective = 'adj', noun = 'n')
mpg
submit()
'I' %p 'love' %p 'R!'
'I' %p% 'love' %p% 'R!'
head(flags)
dim(flags)
viewinfo()
class(flags)
cls_list <- lapply(flags, class)
cls_list
class(cls_list)
as.character(cls_list)
?sapply
cls_vect <- sapply(flags, class)
class(cls_vect)
sum(flags$orange)
flag_colors <- flags[, 11:17]
head(flag_colors)
lapply(flag_colors, sum)
sapply(flag_colors, sum)
lapply(flag_colors, mean)
sapply(flag_colors, mean)
flag_shapes <- flags[, 19:23]
lapply(flag_shapes, range)
shape_mat <- sapply(flag_shapes, range)
shape_mat
class(shape_mat)
unique(c(3, 4, 5, 5, 5, 6, 6))
lapply(flags, unique)
unique_vals <- lapply(flags, unique)
unique_vals
lapply(unique_vals, length)
sapply(unique_vals, length)
sapply(flags, unique)
lapply(unique_vals, function (elem) elem[2])
setwd("~/_WORKING/predix/ml/udm/17_k_mean_clustering")
dataset = read.csv('Mall_Customers.csv')
x = dataset[4:5]
View(x)
View(x)
x = dataset[,4:5]
View(x)
View(x)
# import data
dataset = read.csv('Mall_Customers.csv')
W = dataset[, 4:5]
# using elbow method to find optimal number of cluster
set.seed(6)
wcss = vector()
for (i in 1:10) wcss[i] = sum(kmeans(X, i)$withinss)
plot(1:10, wcss, type = 'b', main = paste('Cluster of Clients'))
wcss = vector()
for (i in 1:10) wcss[i] = sum(kmeans(X, i)$withinss)
plot(1:10, wcss, type = 'b', main = paste('Cluster of Clients'))
# import data
dataset = read.csv('Mall_Customers.csv')
X = dataset[, 4:5]
# using elbow method to find optimal number of cluster
set.seed(6)
wcss = vector()
for (i in 1:10) wcss[i] = sum(kmeans(X, i)$withinss)
plot(1:10, wcss, type = 'b', main = paste('Cluster of Clients'))
set.seed(26)
kmeans = kmeans(X, 5, iter.max = 300, nstart = 10)
library(cluster)
# import data
dataset = read.csv('Mall_Customers.csv')
X = dataset[, 4:5]
# using elbow method to find optimal number of cluster
set.seed(6)
wcss = vector()
for (i in 1:10) wcss[i] = sum(kmeans(X, i)$withinss)
plot(1:10, wcss, type = 'b', main = paste('Cluster of Clients'))
# apply k-means
set.seed(26)
kmeans = kmeans(X, 5, iter.max = 300, nstart = 10)
# vsualizing
library(cluster)
clusplot(
X,
kmeans$cluster,
lines = 0,
shade = TRUE,
color = TRUE,
labels = 2,
plotchar = FALSE,
span = TRUE,
main = 'Cluster of client',
xlab = 'Income',
ylab = 'Score'
)
setwd("~/_WORKING/predix/ml/udm/18_hierachical_clustering")
# Hierarchical Clustering
# Importing the dataset
dataset = read.csv('Mall_Customers.csv')
dataset = dataset[4:5]
# Using the dendrogram to find the optimal number of clusters
dendrogram = hclust(d = dist(dataset, method = 'euclidean'), method = 'ward.D')
plot(dendrogram,
main = paste('Dendrogram'),
xlab = 'Customers',
ylab = 'Euclidean distances')
# Fitting Hierarchical Clustering to the dataset
hc = hclust(d = dist(dataset, method = 'euclidean'), method = 'ward.D')
y_hc = cutree(hc, 5)
# Visualising the clusters
library(cluster)
clusplot(dataset,
y_hc,
lines = 0,
shade = TRUE,
color = TRUE,
labels= 2,
plotchar = FALSE,
span = TRUE,
main = paste('Clusters of customers'),
xlab = 'Annual Income',
ylab = 'Spending Score')
# Importing the dataset
dataset = read.csv('Mall_Customers.csv')
dataset = dataset[4:5]
dist(X, method = 'euclidean')
d = dist(X, method = 'euclidean')
dist(X, method = 'euclidean')
dentogram = hclust(dist(X, method = 'euclidean'), method = 'ward.D')
rm(list = ls())
# Hierarchical Clustering
# Importing the dataset
dataset = read.csv('Mall_Customers.csv')
dataset = dataset[4:5]
# using dentogram to find optimal cluster number
dentogram = hclust(dist(X, method = 'euclidean'), method = 'ward.D')
dentogram = hclust(dist(dataset, method = 'euclidean'), method = 'ward.D')
hclust(dist(dataset, method = 'euclidean'), method = 'ward.D')
# Hierarchical Clustering
# Importing the dataset
dataset = read.csv('Mall_Customers.csv')
dataset = dataset[4:5]
# using dentogram to find optimal cluster number
dentogram = hclust(dist(dataset, method = 'euclidean'), method = 'ward.D')
plot(dentogram)
y_hc = cutree(dentogram, k = 6)
# Hierarchical Clustering
# Importing the dataset
dataset = read.csv('Mall_Customers.csv')
dataset = dataset[4:5]
# using dentogram to find optimal cluster number
dentogram = hclust(dist(dataset, method = 'euclidean'), method = 'ward.D')
plot(dentogram)
# fitting model
y_hc = cutree(dentogram, k = 6)
# vsualizing
library(cluster)
clusplot(
X,
y_hc,
lines = 0,
shade = TRUE,
color = TRUE,
labels = 2,
plotchar = FALSE,
span = TRUE,
main = 'Cluster of client',
xlab = 'Income',
ylab = 'Score'
)
# Hierarchical Clustering
# Importing the dataset
dataset = read.csv('Mall_Customers.csv')
dataset = dataset[4:5]
# using dentogram to find optimal cluster number
dentogram = hclust(dist(dataset, method = 'euclidean'), method = 'ward.D')
plot(dentogram)
# fitting model
y_hc = cutree(dentogram, k = 6)
# vsualizing
library(cluster)
clusplot(
dataset,
y_hc,
lines = 0,
shade = TRUE,
color = TRUE,
labels = 2,
plotchar = FALSE,
span = TRUE,
main = 'Cluster of client',
xlab = 'Income',
ylab = 'Score'
)
# Hierarchical Clustering
# Importing the dataset
dataset = read.csv('Mall_Customers.csv')
X = dataset[4:5]
# using dentogram to find optimal cluster number
dentogram = hclust(dist(X, method = 'euclidean'), method = 'ward.D')
plot(dentogram)
# fitting model
y_hc = cutree(dentogram, k = 6)
# vsualizing
library(cluster)
clusplot(
X,
y_hc,
lines = 0,
shade = TRUE,
color = TRUE,
labels = 2,
plotchar = FALSE,
span = TRUE,
main = 'Cluster of client',
xlab = 'Income',
ylab = 'Score'
)
# Hierarchical Clustering
# Importing the dataset
dataset = read.csv('Mall_Customers.csv')
X = dataset[4:5]
# using dentogram to find optimal cluster number
dentogram = hclust(dist(X, method = 'euclidean'), method = 'ward.D')
plot(dentogram)
# fitting model
y_hc = cutree(dentogram, k = 5)
# vsualizing
library(cluster)
clusplot(
X,
y_hc,
lines = 0,
shade = TRUE,
color = TRUE,
labels = 2,
plotchar = FALSE,
span = TRUE,
main = 'Cluster of client',
xlab = 'Income',
ylab = 'Score'
)
setwd("~/_WORKING/predix/ml/udm/19_apriori")
rm(list = ls())
clear
dataset = read.csv("Market_Basket_Optimisation.csv")
View(dataset)
View(dataset)
dataset = read.csv("Market_Basket_Optimisation.csv", header = FALSE)
View(dataset)
View(dataset)
install.packages('arules')
library(arules)
dataset = read.transactions("Market_Basket_Optimisation.csv", sep = ',')
dataset = read.csv("Market_Basket_Optimisation.csv", header = FALSE)
source('~/_WORKING/predix/ml/udm/19_apriori/apriori.R', echo=TRUE)
View(dataset_raw)
View(dataset_raw)
dataset = read.transactions("Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequency(dataset, topN = 100)
itemFrequencyPlot(dataset, topN = 100)
itemFrequencyPlot(dataset, topN = 10)
itemFrequencyPlot(dataset, topN = 10)
itemFrequencyPlot(dataset, topN = 100)
rules = apriori(
dataset,
parameter = list(support = 0.003, confidence = 0.8)
)
summary(rules)
rules = apriori(
dataset,
parameter = list(support = 0.003, confidence = 0.8)
)
# install.packages('arules')
library(arules)
dataset_raw = read.csv("Market_Basket_Optimisation.csv", header = FALSE)
# create sparse matrix
dataset = read.transactions("Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN = 100)
# training apriori on data set
rules = apriori(
data = dataset,
parameter = list(support = 0.003, confidence = 0.8)
)
summary(rules)
rules = apriori(
data = dataset,
parameter = list(support = 0.003, confidence = 0.8)
)
rules
# install.packages('arules')
library(arules)
dataset_raw = read.csv("Market_Basket_Optimisation.csv", header = FALSE)
# create sparse matrix
dataset = read.transactions("Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN = 100)
# training apriori on data set
rules = apriori(
data = dataset,
parameter = list(support = 0.003, confidence = 0.1)
)
rules
# install.packages('arules')
library(arules)
dataset_raw = read.csv("Market_Basket_Optimisation.csv", header = FALSE)
# create sparse matrix
dataset = read.transactions("Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN = 100)
# training apriori on data set
rules = apriori(
data = dataset,
parameter = list(support = 0.003, confidence = 0.4)
)
summary(rules)
inspect(rules[1:10])
inspect(sort(rules, by = 'lift')[1:10])
# install.packages('arules')
library(arules)
dataset_raw = read.csv("Market_Basket_Optimisation.csv", header = FALSE)
# create sparse matrix
dataset = read.transactions("Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN = 100)
# training apriori on data set
rules = apriori(
data = dataset,
parameter = list(support = 0.003, confidence = 0.7)
)
summary(rules)
# visualizing
inspect(sort(rules, by = 'lift')[1:10])
# install.packages('arules')
library(arules)
dataset_raw = read.csv("Market_Basket_Optimisation.csv", header = FALSE)
# create sparse matrix
dataset = read.transactions("Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN = 100)
# training apriori on data set
rules = apriori(
data = dataset,
parameter = list(support = 0.003, confidence = 0.2)
)
summary(rules)
# visualizing
inspect(sort(rules, by = 'lift')[1:10])
# install.packages('arules')
library(arules)
dataset_raw = read.csv("Market_Basket_Optimisation.csv", header = FALSE)
# create sparse matrix
dataset = read.transactions("Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN = 100)
# training apriori on data set
rules = apriori(
data = dataset,
parameter = list(support = 0.003, confidence = 0.1)
)
summary(rules)
# visualizing
inspect(sort(rules, by = 'lift')[1:10])
# install.packages('arules')
library(arules)
dataset_raw = read.csv("Market_Basket_Optimisation.csv", header = FALSE)
# create sparse matrix
dataset = read.transactions("Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN = 100)
# training apriori on data set
rules = apriori(
data = dataset,
parameter = list(support = 0.004, confidence = 0.1)
)
summary(rules)
# visualizing
inspect(sort(rules, by = 'lift')[1:10])
# install.packages('arules')
library(arules)
dataset_raw = read.csv("Market_Basket_Optimisation.csv", header = FALSE)
# create sparse matrix
dataset = read.transactions("Market_Basket_Optimisation.csv", sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN = 100)
# training apriori on data set
rules = apriori(
data = dataset,
parameter = list(support = 0.004, confidence = 0.2)
)
summary(rules)
# visualizing
inspect(sort(rules, by = 'lift')[1:10])
# Eclat
# Data Preprocessing
# install.packages('arules')
library(arules)
dataset = read.csv('Market_Basket_Optimisation.csv')
dataset = read.transactions('Market_Basket_Optimisation.csv', sep = ',', rm.duplicates = TRUE)
summary(dataset)
itemFrequencyPlot(dataset, topN = 10)
# Training Eclat on the dataset
rules = eclat(data = dataset, parameter = list(support = 0.003, minlen = 2))
# Visualising the results
inspect(sort(rules, by = 'support')[1:10])
itemFrequencyPlot(dataset, topN = 10)
rules = eclat(data = dataset, parameter = list(support = 0.003, minlen = 2))
setwd("~/_WORKING/predix/ml/udm/21_upper_confidence_bound")
dataset = read.csv('Ads_CTR_Optimisation.csv')
dataset = read.csv('Ads_CTR_Optimisation.csv')
# random selection
N = 10000
d = 10
ads_selected = integer(0)
total_reward = 0
for (n in 1:N) {
ad = sample(1:10, 1)
ads_selected = append(ads_selected, ad)
reward = dataset[n, ad]
total_reward = total_reward + reward
}
ads_selected
dataset = read.csv('Ads_CTR_Optimisation.csv')
# random selection
N = 10000
d = 10
ads_selected = integer(0)
total_reward = 0
for (n in 1:N) {
ad = sample(1:10, 1)
ads_selected = append(ads_selected, ad)
reward = dataset[n, ad]
total_reward = total_reward + reward
}
# Visualising the results
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections',
xlab = 'Ads',
ylab = 'Number of times each ad was selected')
dataset = read.csv('Ads_CTR_Optimisation.csv')
# UCB
N = 10000
d = 10
ads_selected = integer(0)
total_reward = 0
number_of_selections = integer(d)
sum_of_rewards = integer(d)
for (n in 1:N) {
ad = 0
max_ucb = 0
for (i in 1:d) {
if (number_of_selections[i] > 0) {
avg_reward = sum_of_rewards[i] / number_of_selections[i]
delta_i = sqrt(3/2 * log(n) / number_of_selections[i])
ucb = avg_reward + delta_i
} else {
ucb = 1e400
}
if (ucb > max_ucb) {
max_ucb = ucb
ad = i
}
}
ads_selected = append(ads_selected, ad)
number_of_selections[ad] = number_of_selections[ad] + 1
reward = dataset[n, ad]
sum_of_rewards[ad] = sum_of_rewards[ad] + reward
total_reward = total_reward + reward
}
# random selection
# N = 10000
# d = 10
# ads_selected = integer(0)
# total_reward = 0
# for (n in 1:N) {
#   ad = sample(1:10, 1)
#   ads_selected = append(ads_selected, ad)
#   reward = dataset[n, ad]
#   total_reward = total_reward + reward
# }
# Visualising the results
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections',
xlab = 'Ads',
ylab = 'Number of times each ad was selected')
