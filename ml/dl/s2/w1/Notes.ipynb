{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical aspects of Deep Learning\n",
    "\n",
    "- Recall that different types of initializations lead to different results\n",
    "- Recognize the importance of initialization in complex neural networks.\n",
    "- Recognize the difference between train/dev/test sets\n",
    "- Diagnose the bias and variance issues in your model\n",
    "- Learn when and how to use regularization methods such as dropout or L2 regularization.\n",
    "- Understand experimental issues in deep learning such as Vanishing or Exploding gradients and learn how to deal with them\n",
    "- Use gradient checking to verify the correctness of your backpropagation implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up your Machine Learning Application \n",
    "\n",
    "### Train / Dev / Test sets\n",
    "\n",
    "** Applied machine learning is a highly iterative process **\n",
    "\n",
    "- number of layers\n",
    "- number of hidden units\n",
    "- learning rate\n",
    "- activation function\n",
    "- ...\n",
    "\n",
    "Imposible to guess the right hyper parameters at the first time. By train and test the algorithms and model friquenly and tune hyper parameters, we can come out with the optimal hyper parameters which provide best performance.\n",
    "\n",
    "** Train / Dev / Test sets **\n",
    "- Split dataset to train set, cross validation (dev) set and test set, where train set use to train the model, dev set to test the algorithm and tune hyper parameters once we finished with the final model, we will test it with the test set.\n",
    "- Previous era (100 - 1,000 - 10,000): 70/30 (train/test) or 60/20/20 (train/dev/test)\n",
    "- Big data era (> 1,000,000): 10,000 dev and 10,000 test --> 98/1/1 or 99.5/0.25-0.4/0.1-0.25 (train/dev/test)\n",
    "\n",
    "** Mismatched train/test distribution **\n",
    "\n",
    "For example we are building an application which allow users to upload pictures and the app will recognize cats in those picstures.\n",
    "- We conduct the train set of the cat pitures from web pages which have high quality and resolution\n",
    "- The dev and test set were uploaded by user which have low quality and resolution (maybe blur, etc.)\n",
    "\n",
    "$\\Rightarrow$ Make sure the train and test sets came from the same distribution\n",
    "\n",
    "Not having the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias / Variance\n",
    "\n",
    "- high bias: under fitting\n",
    "- high variance: over fitting\n",
    "- Human error or Optimal (Bayes) error, for example a very blur picture will have the Bayes error very high or even human cannot regconize it, for high quality picture it is nearly zero.\n",
    "- Compare the Bayes error with the train set error to determine high the bias problem\n",
    "- Compare the dev/test set error with train set error to determine the high variance probem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Recipe for Machine Learning\n",
    "\n",
    "** Check high bias? (train set performance) **\n",
    "- _Bigger network_\n",
    "- Train longer (more iteration)\n",
    "- New NN architecture\n",
    "\n",
    "** Check high variance? (dev/test set performance) **\n",
    "- _More data_\n",
    "- Regularization\n",
    "- New NN architecture\n",
    "\n",
    "** Bias vs. Variance trade off **\n",
    "- More data will not hurt bias\n",
    "- Bigger network will not hurt variance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizing your neural network\n",
    "\n",
    "### Regularization\n",
    "\n",
    "** Logistic Regression **\n",
    "\n",
    "- We try to minimize:\n",
    "$$\n",
    "agrmin \\ J(w, b) \\ where \\ w \\in \\mathbb{R}^n_x \\ , \\ b \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "with:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)})\n",
    "$$\n",
    "\n",
    "we will add regularization to pinalize the weights:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m} \\|w\\|^2_2\n",
    "$$\n",
    "\n",
    "where the L2 norm of w is:\n",
    "\n",
    "$$\n",
    "\\|w\\|^2_2 = \\sum_{j=1}^{n_x} w_j^2 = w^tw\n",
    "$$\n",
    "\n",
    "$\\Rightarrow$ This called L2 regularization\n",
    "\n",
    "- We can use L1 regularization using L1 norm of w, where the w will be sparse, normally it won't help much, the L2 regularizaton is used much more often:\n",
    "\n",
    "$$\n",
    "\\frac{\\lambda}{m} \\|w\\|_1 = \\frac{\\lambda}{m}\\sum_{j=1}^{n_x}|w_j|\n",
    "$$\n",
    "\n",
    "- $\\lambda$ called the regularization parameter\n",
    "\n",
    "** Neural Network **\n",
    "\n",
    "- We have lost function $L$ layers and $m$ samples in data set with additional regularization to pinalize the weight matrix:\n",
    "\n",
    "$$\n",
    "J(w^{[1]}, b^{[1]}, w^{[2]}, b^{[2]}, ..., w^{[L]}, b^{[L]}) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m} \\sum_{l=1}^L \\|W^{[l]}\\|^2_F\n",
    "$$\n",
    "\n",
    "where the Frobenius norm (L2) of matrix $W^{[l]}$ is:\n",
    "\n",
    "$$\n",
    "\\|W^{[l]}\\|^2_F = \\sum_i^{n^{[l-1]}}\\sum_j^{n^{[l]}}(W_{ij}^{[l]})^2 \\ because \\ W \\ is \\ a \\ [n^{[l-1]} \\times n^{[l]}] \\ matrix\n",
    "$$\n",
    "\n",
    "- Gradient decent with regularization\n",
    "\n",
    "$$\n",
    "dw^{[l]} = \\frac{\\partial{J}}{\\partial{W^{[l]}}} = (backprob) + \\frac{\\lambda}{m} W^{[l]} \n",
    "$$\n",
    "\n",
    "- Update parameters with regularization\n",
    "\n",
    "$$\n",
    "W^{[l]} = W^{[l]} - \\alpha dw^{[l]} \n",
    "= W^{[l]} - \\alpha ((backprob) + \\frac{\\lambda}{m} W^{[l]}) \\\\\n",
    "= W^{[l]} - \\frac{\\lambda\\alpha}{m} W^{[l]} - \\alpha(backprob) \\\\\n",
    "= W^{[l]}(1 - \\frac{\\lambda\\alpha}{m}) - \\alpha(backprob)\n",
    "$$\n",
    "\n",
    "this called \"Weight decay\" where we multiply the $W^{[l]}$ by $(1 - \\frac{\\lambda\\alpha}{m})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Why regularization reduces overfitting?\n",
    "- Lost function:\n",
    "\n",
    "$$\n",
    "J(w^{[1]}, b^{[1]}, w^{[2]}, b^{[2]}, ..., w^{[L]}, b^{[L]}) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m} \\sum_{l=1}^L \\|W^{[l]}\\|^2_F\n",
    "$$\n",
    "\n",
    "- With $\\lambda$ very large we can make the $W^{[l]}$ close to zero, and mininize the impact of the hidden units to have a simpler network\n",
    "- If we use $tanh$ as the activation function $g(z) = tanh(z)$, if $z$ very small close to zero, the $g(z)$ will be nearly linear\n",
    "- When we increase $\\lambda$, the W will decrease close to zero, with $z = Wx + b$, by decrease $W$, we will have $z$ very small so the $g(z)$ will be nearly linear\n",
    "- The activations will be nearly equa the linear function, we will have the deep network with linear activation functions in every layer\n",
    "- Remember with debug the SGD, we need to plot the cost function with the regularization term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Regularization\n",
    "\n",
    "- Go throught the layers\n",
    "- Set probabilities of eliminating a node (eg. keep 80%, drop 20%)\n",
    "- Have a smaller network\n",
    "\n",
    "**Implementing dropout (Inverted dropout)**\n",
    "\n",
    "with layer $l=3$\n",
    "\n",
    "```python\n",
    "keep_prob = 0.8\n",
    "d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob\n",
    "a3 = a3 * d3\n",
    "a3 = a3 / keep_prob # to not reduce the value of z by dropout 20%\n",
    "```\n",
    "- Do not dropout at the test time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other regularization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up your optimization problem\n",
    "\n",
    "### Normalizing inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing / Exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight initialization for deep network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical approximation of gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient checking implementation notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
