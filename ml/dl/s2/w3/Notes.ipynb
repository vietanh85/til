{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning, Batch Normalization and Programming Frameworks\n",
    "\n",
    "## Hyperparameter tuning\n",
    "\n",
    "### Tuning process\n",
    "\n",
    "**Hyperparameters**\n",
    "- $\\alpha$\n",
    "- $\\beta$\n",
    "- mini-batch size\n",
    "- no. of hidden unit\n",
    "- no. of layers\n",
    "- learning rate decay\n",
    "- $\\beta_1, \\beta_2, \\epsilon$\n",
    "\n",
    "Use random sampling and plot them to the chart\n",
    "\n",
    "### Using an appropriate scale to pick hyperparameters\n",
    "\n",
    "**Appropriate scale for Learning rate $\\alpha$**\n",
    "\n",
    "$\n",
    "use \\ logarithmic \\ r = log(\\alpha) \\\\\n",
    "\\alpha \\in [0.0001, 1]\n",
    "r \\in [-4, 0] \\\\\n",
    "\\rightarrow \\alpha = 10^r\n",
    "$\n",
    "\n",
    "**Hyperparameters for exponentially weighted averages $\\beta$**\n",
    "\n",
    "$\n",
    "\\beta \\in [0.9, 0.999] \\\\\n",
    "(1 - \\beta) \\in [0.001, 0.1] \\\\\n",
    "r \\in [-3, -1] \\\\\n",
    "\\rightarrow (1-\\beta) = 10^r \\rightarrow \\beta = 1 - 10^r\n",
    "$\n",
    "\n",
    "### Hyperparameters tuning in practice: Pandas vs. Caviar\n",
    "\n",
    "- Re-test hyperparameters occasionally\n",
    "- Pandas approach: Babysitting one model at a time: tune hyperparameters one day at a time if we don't have computation capacity to train multiple models at a time\n",
    "- Caviar approach: Training many models in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "### Normalizing activations in a network\n",
    "\n",
    "**Input normalization in logistic regression**\n",
    "\n",
    "Subtract mean:\n",
    "\n",
    "$\n",
    "\\mu = \\frac{1}{m}\\sum_i X^{(i)} \\\\\n",
    "X = X - \\mu\n",
    "$\n",
    "\n",
    "Normalize variance:\n",
    "\n",
    "$\n",
    "\\sigma^2 = \\frac{1}{m}\\sum_i X^{(i)2} \\\\\n",
    "X = \\frac{X}{\\sigma^2}\n",
    "$\n",
    "\n",
    "**Normalization for hidden layers**\n",
    "\n",
    "Can normalize for both Z and A, but Z is more often\n",
    "\n",
    "**Batch Norm**\n",
    "\n",
    "Given $z^{(1)}, z^{(2)}, ..., z^{(m)}$ in layer $l$ of NN\n",
    "\n",
    "$\n",
    "\\mu = \\frac{1}{m}\\sum_i z^{(i)} \\\\\n",
    "\\sigma^2 = \\frac{1}{m}\\sum_i (z^{(i)} - \\mu)^2 \\\\\n",
    "z^{(i)}_{norm} = \\frac{z^{(i)} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\\\\n",
    "\\tilde{z^{(i)}} = \\gamma z^{(i)}_{norm} + \\beta \\\\\n",
    "$\n",
    "\n",
    "where $\\gamma, \\beta$ are learnable parameters of the models\n",
    "\n",
    "$\n",
    "if: \\\\\n",
    "\\ \\ \\ \\ \\gamma = \\sqrt{\\sigma^2 + \\epsilon}\\\\\n",
    "\\ \\ \\ \\ \\beta = \\mu \\\\\n",
    "then: \\tilde{z^{(i)}} = z^{(i)}\n",
    "$\n",
    "\n",
    "### Fitting Batch Norm into a neural network\n",
    "\n",
    "$\n",
    "X \\xrightarrow{W^{[1]}, b^{[1]}} Z^{[1]} \\xrightarrow[Batch \\ Norm \\ (BN)]{\\beta^{[1]}, \\gamma^{[1]}} \\tilde{Z^{[1]}} \\rightarrow A^{[1]} = g^{[1]}(\\tilde{Z^{[1]}}) \\xrightarrow{W^{[2]}, b^{[2]}} Z^{[2]} \\xrightarrow[Batch \\ Norm \\ (BN)]{\\beta^{[2]}, \\gamma^{[2]}} \\tilde{Z^{[2]}} \\rightarrow A^{[2]} = g^{[2]}(\\tilde{Z^{[2]}}) \\rightarrow ...\n",
    "$\n",
    "\n",
    "Parameters: $W, b, \\beta, \\gamma$\n",
    "\n",
    "Using Tensorflow\n",
    "\n",
    "```python\n",
    "# using tensorflow\n",
    "tf.nn.batch_normallization\n",
    "```\n",
    "\n",
    "Normally, batch norm will be applied with mini-batch\n",
    "\n",
    "When use batch norm, we can omit $b$\n",
    "\n",
    "$\n",
    "z^{[l]} = W^{[l]}a^{[l-1]}\n",
    "$\n",
    "\n",
    "Dimension of $z^{[l]}$ is $(n^{[l]} \\times 1) \\Rightarrow $ dimension of $\\beta^{[l]} \\ and \\ \\gamma^{[l]}$ is $(n^{[l]} \\times 1)$ \n",
    "\n",
    "**Implementing gradient descent**\n",
    "\n",
    "$\n",
    "for \\ t = 1 ... no. \\ of \\ mini-batches: \\\\\n",
    "\\ \\ \\ \\ Compute \\ forwardprop \\ on \\ X^{\\{t\\}} \\\\\n",
    "\\ \\ \\ \\ \\ \\ \\ \\ In \\ each \\ layer, \\ use \\ BN \\ to \\ replace \\ Z^{[l]} \\ with \\ \\tilde{Z^{[l]}} \\\\\n",
    "\\ \\ \\ \\ Use \\ backprop \\ to \\ compute \\ dW^{[l]}, db^{[l]}, d\\beta^{[l]}, d\\gamma^{[l]} \\\\\n",
    "\\ \\ \\ \\ Update \\ parameters \\\\\n",
    "$\n",
    "\n",
    "Work with momentum, RMSProp, Adam, etc.\n",
    "\n",
    "### Why does Batch Norm work?\n",
    "\n",
    "- Covariance shift\n",
    "\n",
    "### Batch Norm at test time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Muti-class classification\n",
    "\n",
    "### Softmax regression\n",
    "\n",
    "$\n",
    "C = no. \\ of \\ classes \\\\\n",
    "n^{[L]} = C\n",
    "$\n",
    "\n",
    "**Softmax layer**\n",
    "\n",
    "$\n",
    "Z^{[L]} = W^{[L]}a^{[L-1]} + b^{[L]} \\\\\n",
    "Activation \\ function: \\\\\n",
    "t = e^{(z^{[L]})} \\\\\n",
    "a^{[L]} = \\frac{t_i}{\\sum_{t=0}^C t_i} = \\frac{e^{(z^{[L]}_i)}}{\\sum_{t=0}^C e^{(z^{[L]}_i)}}\n",
    "$\n",
    "\n",
    "### Training a softmax classifier\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming framework\n",
    "\n",
    "### Deep learning frameworks\n",
    "- Caffe/Caffe2\n",
    "- CNTK\n",
    "- DL4J\n",
    "- Keras\n",
    "- Lasagne\n",
    "- mxnet\n",
    "- PaddlePaddle\n",
    "- Tensorflow\n",
    "- Theano\n",
    "- Torch/PyTorch\n",
    "\n",
    "**Choosing deep learning frameworks:**\n",
    "\n",
    "- Ease of programming (development and deployment)\n",
    "- Running speed\n",
    "- Truly open (open source with good governance)\n",
    "\n",
    "### Tensorflow\n",
    "\n",
    "**Motivating problem**\n",
    "\n",
    "We have cost function to minimize $J(w) = w^2 - 10w + 25$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "coefficients = np.array([[1.], [-20.], [100.]])\n",
    "\n",
    "w = tf.Variable(0, dtype=tf.float32)\n",
    "x = tf.placeholder(tf.float32, [3, 1])\n",
    "\n",
    "# cost = tf.add(tf.add(w**2, tf.multiply(-10., w)), 25)\n",
    "# cost = w**2 - 10*w + 25\n",
    "# cost = (w-5)**2\n",
    "cost = x[0][0]*w**2 + x[1][0]*w + x[2][0]\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.99998\n"
     ]
    }
   ],
   "source": [
    "# automatically compute backprop by computing forwardprop\n",
    "session.run(train, feed_dict={x: coefficients})\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.99998\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    session.run(train, feed_dict={x: coefficients})\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
