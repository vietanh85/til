{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization algorithms\n",
    "\n",
    "Learning Objectives\n",
    "\n",
    "- Remember different optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam\n",
    "- Use random minibatches to accelerate the convergence and improve the optimization\n",
    "- Know the benefits of learning rate decay and apply it to your optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent\n",
    "\n",
    "- Vectorization allows to efficiently compute on m example\n",
    "- What if m = 5,000,000?\n",
    "- 5,000 batch with 1,000 each $X^{\\{t\\}}, Y^{\\{t\\}}$\n",
    "- This is \"one epoch\" of training:\n",
    "\n",
    "$\n",
    "repeat \\ \\{ \\\\\n",
    "for \\ t = 1, ..., 5000: \\\\\n",
    "\\ \\ \\ \\ forward \\ prob \\ on \\ X^{\\{t\\}} \\\\\n",
    "\\ \\ \\ \\ \\ \\ \\ \\ Z^{[1]} = W^{[1]} X^{\\{1\\}} + b^{[1]} \\\\\n",
    "\\ \\ \\ \\ \\ \\ \\ \\ A^{[1]} = g^{[1]}(Z^{[1]}) \\\\\n",
    "\\ \\ \\ \\ \\ \\ \\ \\ Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]} \\\\\n",
    "\\ \\ \\ \\ \\ \\ \\ \\ A^{[2]} = g^{[2]}(Z^{[2]}) \\\\\n",
    "\\ \\ \\ \\ \\ \\ \\ \\ ... \\\\\n",
    "\\ \\ \\ \\ \\ \\ \\ \\ A^{[L]} = g^{[L]}(Z^{[L]}) \\\\\n",
    "\\ \\ \\ \\ compute \\ cost \\ funciton \\ \\mathcal{J}^{\\{t\\}} = \\frac{1}{1000} \\sum_{i=1}^{1000} \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2 \\times 1000} \\sum_{l=1}^{L} \\|W^{[l]}\\|_F^2 \\\\\n",
    "\\ \\ \\ \\ backprob \\ to \\ compute \\ gradients \\ w.r.t \\ \\mathcal{J}^{\\{t\\}} \\ using \\ X^{\\{t\\}}, Y^{\\{t\\}} to \\ update \\ W \\ and \\ b \\\\\n",
    "\\}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand mini-batch gradient descent\n",
    "\n",
    "- mini-batch = m: Batch gradient descent. $(X^{\\{1\\}}, X^{\\{1\\}}) = (X, Y) \\rightarrow$ too long per iteration\n",
    "- mini-batch = 1: Stochastic gradient decent. $(X^{\\{1\\}}, X^{\\{1\\}}) = (x^{(1)}, y^{(1)}) \\rightarrow$ lose the speedup from vectorization\n",
    "- in practice: mini-batch is between 1 and m (not too big or too small) $\\rightarrow$ fastest learning \n",
    "\n",
    "**Choosing mini-batch size**\n",
    "- If small training set (m < 2,000): use batch gradient descent\n",
    "- Typical mini-batch size: 64, 128, 256, 512, 1024\n",
    "- Make sure mini-batch fit in CPU/GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exponentially weighted averages\n",
    "\n",
    "$\n",
    "v_t = \\beta v_{t-1} + (1 - \\beta)\\theta_t \\\\\n",
    "\\ \\ \\beta = 0.9: last \\ 10 \\ days: \\frac{1}{1-\\beta} \\ days \\\\\n",
    "\\ \\ \\beta = 0.98: last \\ 50 \\ days: \\frac{1}{1-\\beta} \\ days \\\\\n",
    "\\ \\ \\beta = 0.5: last \\ 2 \\ days: \\frac{1}{1-\\beta} \\ days \\\\\n",
    "$\n",
    "\n",
    "### Understand exponentially weighted averages\n",
    "\n",
    "$\n",
    "v_t = \\beta v_{t-1} + (1 - \\beta)\\theta_t \\\\\n",
    "v_0 = 0 \\\\\n",
    "v_1 = \\beta v_0 + (1 - \\beta)\\theta_1 \\\\\n",
    "v_2 = \\beta v_1 + (1 - \\beta)\\theta_2 \\\\\n",
    "... \\\\\n",
    "$\n",
    "\n",
    "### Bias correction in exponentially weighted averages\n",
    "$\n",
    "v_t = \\frac{v_t}{1 - \\beta^t}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent with momentum\n",
    "\n",
    "$\n",
    "Momentum: \\\\\n",
    "v_{dW} = v_{db} = 0 \\\\\n",
    "On \\ iteration \\ t: \\\\\n",
    "\\ \\ \\ \\ Compute \\ dW, \\ db, \\ on \\ current \\ mini-batch \\\\\n",
    "\\ \\ \\ \\ v_{dW} = \\beta v_{dW} + (1 - \\beta)dW \\\\\n",
    "\\ \\ \\ \\ v_{db} = \\beta v_{db} + (1 - \\beta)db \\\\\n",
    "\\ \\ \\ \\ Update \\ W \\ and \\ b \\\\\n",
    "\\ \\ \\ \\ W = W - \\alpha v_{dW} \\\\\n",
    "\\ \\ \\ \\ b = b - \\alpha v_{db} \\\\\n",
    "Hyperparameter: \\ \\alpha, \\beta \\ \\ \\ \\ \\beta=0.9 \\ \\approx \\ last \\ 10 \\ iterations \\\\\n",
    "Sometimes \\ omit \\ (1 - \\beta)\n",
    "$\n",
    "\n",
    "### RMSprop (Root Mean Square prop)\n",
    "\n",
    "$\n",
    "RMS: \\\\\n",
    "On \\ iteration \\ t: \\\\\n",
    "\\ \\ \\ \\ Compute \\ dW, \\ db, \\ on \\ current \\ mini-batch \\\\\n",
    "\\ \\ \\ \\ s_{dW} = \\beta s_{dW} + (1 - \\beta)dW^2 \\leftarrow small \\\\\n",
    "\\ \\ \\ \\ s_{db} = \\beta s_{db} + (1 - \\beta)db^2 \\leftarrow large \\\\\n",
    "\\ \\ \\ \\ Update \\ W \\ and \\ b \\\\\n",
    "\\ \\ \\ \\ W = W - \\alpha \\frac{dW}{\\sqrt{s_{dW} + \\epsilon}} \\\\\n",
    "\\ \\ \\ \\ b = b - \\alpha \\frac{db}{\\sqrt{s_{db} + \\epsilon}} \\\\\n",
    "Hyperparameter: \\ \\alpha, \\beta, \\epsilon = 10^{-8} \\\\\n",
    "$\n",
    "\n",
    "### Adam optimization algorithm\n",
    "\n",
    "$\n",
    "Adam: \\\\\n",
    "v_{dW} = v_{db} = s_{dW} = s_{db} = 0 \\\\\n",
    "On \\ iteration \\ t: \\\\\n",
    "\\ \\ \\ \\ Compute \\ dW, \\ db, \\ on \\ current \\ mini-batch \\\\\n",
    "\\ \\ \\ \\ v_{dW} = \\beta_1 v_{dW} + (1 - \\beta)dW,\n",
    "\\ \\ \\ \\ v_{db} = \\beta_1 v_{db} + (1 - \\beta)db \\leftarrow \\ Momentum \\\\\n",
    "\\ \\ \\ \\ s_{dW} = \\beta_2 s_{dW} + (1 - \\beta)dW^2,\n",
    "\\ \\ \\ \\ s_{db} = \\beta_2 s_{db} + (1 - \\beta)db^2 \\leftarrow \\ RMS\\\\\n",
    "\\ \\ \\ \\ v^{corrention}_{dW} = \\frac{v_{dW}}{1 - \\beta_1^t},\n",
    "\\ \\ \\ \\ v^{corrention}_{db} = \\frac{v_{db}}{1 - \\beta_1^t}, \\leftarrow \\ Bias \\ correction \\\\\n",
    "\\ \\ \\ \\ s^{corrention}_{dW} = \\frac{s_{dW}}{1 - \\beta_2^t},\n",
    "\\ \\ \\ \\ s^{corrention}_{db} = \\frac{s_{db}}{1 - \\beta_2^t}, \\leftarrow \\ Bias \\ correction \\\\\n",
    "\\ \\ \\ \\ Update \\ W \\ and \\ b \\\\\n",
    "\\ \\ \\ \\ W = W - \\alpha \\frac{v^{corrention}_{dW}}{\\sqrt{s^{corrention}_{dW} + \\epsilon}} \\\\\n",
    "\\ \\ \\ \\ b = b - \\alpha \\frac{v^{corrention}_{db}}{\\sqrt{s^{corrention}_{db} + \\epsilon}} \\\\\n",
    "Hyperparameters: \\ \\alpha, \\beta_1, \\beta_2, \\epsilon \\\\\n",
    "$\n",
    "\n",
    "**Hyperparameter choice**\n",
    "- $\\alpha$: need to be tune\n",
    "- $\\beta_1$: default choice is 0.9 (computing moving avegange for $dW$)\n",
    "- $\\beta_2$: default choice is 0.999 (computing moving avegange for $dW^2$)\n",
    "- $\\epsilon$: default choice is $10^{-8}$\n",
    "\n",
    "***Adam: ADAptive Moment estimation***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate decay\n",
    "\n",
    "$\n",
    "\\alpha = \\frac{1}{1 \\ + \\ decay \\ rate \\ \\times \\ epoch \\ num} \\alpha_0\n",
    "$\n",
    "\n",
    "**Other learning rate decay methods**\n",
    "\n",
    "$\n",
    "\\alpha = 0.95^{epoch \\ num} \\times \\alpha_0 \\leftarrow exponentially \\ decay \\\\\n",
    "\\alpha = \\frac{c}{\\sqrt{epoch \\ num}}\\alpha_0 \\ or \\ \\frac{c}{\\sqrt{t}}\\alpha_0\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
